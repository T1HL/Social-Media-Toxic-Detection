# api_server.py
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel, AutoConfig
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from underthesea import word_tokenize
import re
import os

# --- 1. ƒê·ªäNH NGHƒ®A MODEL (Gi·ªëng h·ªát l√∫c Train) ---
class PhoBERT_Classifier(nn.Module):
    def __init__(self, num_labels=2):
        super(PhoBERT_Classifier, self).__init__()
        # Load c·∫•u h√¨nh t·ª´ th∆∞ m·ª•c saved_model ƒë·ªÉ kh√¥ng c·∫ßn m·∫°ng internet
        # N·∫øu l·ªói th√¨ n√≥ s·∫Ω t·ª± t·∫£i l·∫°i t·ª´ HuggingFace
        try:
            self.phobert = AutoModel.from_pretrained("./saved_model")
        except:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y config offline, ƒëang t·∫£i t·ª´ Internet...")
            self.phobert = AutoModel.from_pretrained("vinai/phobert-base-v2")
            
        self.fc = nn.Linear(768, num_labels)

    def forward(self, input_ids, attention_mask):
        features = self.phobert(input_ids=input_ids, attention_mask=attention_mask)
        cls_output = features.last_hidden_state[:, 0, :]
        logits = self.fc(cls_output)
        return logits

# --- 2. KH·ªûI T·∫†O SERVER ---
app = FastAPI()

# Cho ph√©p Web c·ªßa b·∫°n (ch·∫°y ·ªü localhost:3000) g·ªçi v√†o
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- 3. LOAD MODEL & TOKENIZER ---
MODEL_PATH = "./saved_model/phobert_toxic.pth"
TOKENIZER_PATH = "./saved_model"

print("‚è≥ ƒêang kh·ªüi ƒë·ªông AI Server...")
device = torch.device("cpu") # Ch·∫°y tr√™n m√°y t√≠nh c√° nh√¢n d√πng CPU

try:
    # Load Tokenizer t·ª´ c√°c file: vocab.txt, bpe.codes...
    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, local_files_only=True)
    
    # Load Model
    model = PhoBERT_Classifier()
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.to(device)
    model.eval()
    print("‚úÖ Load Model th√†nh c√¥ng! S·∫µn s√†ng nh·∫≠n y√™u c·∫ßu.")
except Exception as e:
    print(f"‚ùå L·ªñI LOAD MODEL: {e}")
    print("üëâ B·∫°n h√£y ki·ªÉm tra l·∫°i xem ƒë√£ b·ªè ƒë·ªß file v√†o th∆∞ m·ª•c 'saved_model' ch∆∞a nh√©!")

# --- 4. H√ÄM X·ª¨ L√ù TEXT ---
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)
    text = word_tokenize(text, format="text") 
    return text

class CommentRequest(BaseModel):
    text: str

@app.post("/predict")
async def predict(item: CommentRequest):
    if not item.text:
        return {"is_toxic": False}

    try:
        # 1. Ti·ªÅn x·ª≠ l√Ω
        clean_text = preprocess_text(item.text)
        
        # 2. M√£ h√≥a (Tokenize)
        encoding = tokenizer(
            clean_text,
            return_tensors='pt',
            max_length=128,
            padding='max_length',
            truncation=True
        )
        
        input_ids = encoding['input_ids'].to(device)
        attention_mask = encoding['attention_mask'].to(device)
        
        # 3. D·ª± ƒëo√°n
        with torch.no_grad():
            outputs = model(input_ids, attention_mask)
            probs = torch.nn.functional.softmax(outputs, dim=1)
            pred_label = torch.argmax(probs, dim=1).item()
            confidence = probs[0][pred_label].item()
            
        return {
            "text": item.text,
            "is_toxic": True if pred_label == 1 else False,
            "confidence": round(confidence, 4)
        }
        
    except Exception as e:
        print(f"L·ªói: {e}")
        raise HTTPException(status_code=500, detail="L·ªói x·ª≠ l√Ω AI")

# L·ªánh ch·∫°y: uvicorn api_server:app --reload